%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% -                       Method                           - %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}

\subsection{Research Approach and Study Design}
The goal is to measure how an H3-keyed middleware cache affects latency and load under skewed spatial workloads, compared to no cache. Each configuration is run with the same datasets, the same mix of query footprints (BBOX and polygons), and the same arrival pattern. Experiments are containerised and scripted so runs are repeatable.

For each workload profile the configurations are cycled through, runs are repeated, and median and percentile metrics are reported across repetitions. External factors are fixed as much as possible (hardware, network, versions, etc.), warm up the system, and then collect measurements during a steady window. The SRE “golden signals” are reported and cache-specific indicators: P50/P95/P99 latency (overall, hit-path, miss-path), requests/s, error rate, saturation (CPU/memory), and hit/miss/partial-hit ratios.

Independent variables are H3 resolution (7–9), decay, and freshness policy. Dependent variables are latency percentiles, PostGIS CPU, Redis memory and evictions, and staleness.

Validity measures include repeated trials, fixed seeds for traffic, and cross-checking results. Threats to validity are noted (dataset representativeness, synthetic vs.\ real traffic, and Docker networking differences), and the evaluation is framed as comparative rather than absolute.

\subsection{System Under Study and Assumptions}
The system mirrors a common geospatial setup: GeoServer exposes WFS/WMS endpoints backed by PostGIS, a middleware proxy sits in front and integrates Redis for caching and Kafka for change events. Prometheus scrapes metrics from services. Clients issue WFS \texttt{GetFeature} requests with BBOX or polygon filters and optional attribute filters.

Layers are published in \texttt{EPSG:4326} and responses are encoded as GeoJSON. Queries target urban layers with realistic skew, mixing small viewports (city blocks) with medium windows (districts). The cache key encodes (layer, SRID, resolution, H3 cell ID, filter hash, version). Each value has a TTL and can be invalidated by events. Redis is configured with a bounded memory and an eviction policy \cite{Redis_Eviction}.

Kafka provides in-partition ordering \cite{Kafka_Introduction}; consumers are in a single group per environment so each partition is processed by exactly one consumer. Event handling is repeat-safe so duplicate invalidations are harmless.

A stable network is assumed during testing, and no background load is assumed except the experiment. GeoServer/PostGIS schemas or indexes are not changed; the middleware is non-invasive and standards-compliant on the wire.

\subsection{Middleware Design and Implementation}
The middleware is a Go service that acts as an HTTP reverse proxy with spatial awareness. It parses incoming requests, maps footprints to H3 cells, looks up Redis for per-cell entries, forwards misses to GeoServer, and composes responses. A Kafka consumer listens for change events, maps updated geometries to cells, and deletes affected keys. The service exposes Prometheus metrics and health checks. Configuration is via environment variables and a simple YAML file; all components run in Docker for reproducibility.

\subsubsection{End-to-End Request Flow and Component Responsibilities}
\textbf{1. Router:} Accepts client WFS/WMS requests, validates required parameters (service, request, typeNames, spatial filter), normalises ECQL/CQL filters, and builds an internal QueryRequest. It preserves wire compatibility with GeoServer so clients do not change their integrations.

\textbf{2. H3 mapper:} Converts the footprint (BBOX as polygon, or user polygon) to a set of H3 cell IDs at the chosen resolution. The selection of resolution is static per run.

\textbf{3. Decision and hotness:} Updates per-cell counters (exponential decay). Cells above threshold are eligible for caching; others pass through without admission to reduce turnover. This keeps memory focused on regions with reuse.

\textbf{4. Cache manager:} Builds composite keys (layer/SRID/res/cell/filter-hash/version), performs a single MGET to Redis, and classifies the request as full hit, partial hit, or miss. Values are stored with TTL. Redis is configured with an eviction policy suitable for caches and a fixed maxmemory to force realistic trade-offs.

\textbf{5. Executor:} For cells not found, constructs a WFS \texttt{GetFeature} URL and forwards to GeoServer. The result is parsed, stored per-cell (with TTL), and streamed to the aggregator.

\textbf{6. Aggregator:} Merges per-cell payloads, deduplicates features by stable identifiers, reapplies the original spatial/attribute filter to avoid overfetch, and returns a single GeoJSON response. It records keys-per-request and merge time.

\textbf{7. Invalidation listener:} Subscribes to Kafka topics carrying change events. For each event, maps the changed geometry to H3 cells and issues key deletions. Processing is idempotent, and ordering is ensured within partitions.

\textbf{8. Metrics/health:} Exposes request counters, latency histograms, hit/miss ratios, Redis stats, and invalidation lag. Health endpoints return liveness/readiness for containers.

\subsubsection{H3 Footprint Mapping and Request Bucketing}
H3’s polygon-to-cells (“polygonToCells”) functions are used to map a request footprint to cell IDs \cite{H3_API_Regions}. A BBOX is treated as a polygon with four vertices. For each request, the mapper returns the set of cell IDs at resolution $r$, which become the unit of reuse and invalidation. The default polygonToCells uses a centroid-in-polygon rule \cite{H3_API_Regions}, which is simple and fast; thin shapes near boundaries can be handled by increasing $r$ or by expanding one ring and trimming on compose.

Resolution controls granularity. Higher $r$ gives smaller cells, more precise coverage, and often higher hit probability for small windows, but increases keys per request. Lower $r$ reduces keys and metadata but can over-cover and reduce locality. Resolution is therefore swept with $r\in\{5,6,7,8,9,10\}$ in experiments and hit ratio, keys-per-request, and bytes-per-cell are recorded.

The cache key includes the cell ID and a stable hash of the attribute filter (normalised ECQL). This keeps semantically different queries isolated while enabling reuse when both footprint and filter match.

On composed responses the original spatial filter is reapplied to remove any features picked up by boundary cells. Multi-polygons are supported by passing outer and inner rings to the polyfill. All cell operations are deterministic so repeated queries map to the same key set.

\subsubsection{Caching in Redis: Key Schema and Result Storage}
The cache entries are stored in Redis using a composite key:
\texttt{<layer>:\allowbreak<srid>:\allowbreak<res>:\allowbreak<cellId>:\allowbreak<filterHash>:\allowbreak<version>}. Values are kept as Redis strings (binary-safe blobs), which allows compact, compressed payloads without concerns about encoding. Each request that spans multiple H3 cells triggers a single \texttt{MGET} for those keys to reduce round trips. \texttt{cellId} is the H3 64-bit identifier \cite{H3_Core_Indexing}, and \texttt{filterHash} fingerprints the normalised ECQL/FES filter so semantically different queries do not collide.

The value layout is simple: a small header (timestamps, byte length, schema/version) and a payload. For vector responses GeoJSON Features or small, per-cell arrays of feature rows are stored. TTL is set at write and attached per key, Redis removes expired keys automatically. Memory pressure is controlled by \texttt{maxmemory} and an eviction policy (e.g., \texttt{allkeys-lru} or \texttt{allkeys-lfu}) so the cache behaves predictably when full.

\subsubsection{Response Composition and Correctness Safeguards}
The aggregator merges per-cell payloads into one \texttt{FeatureCollection}. De-duplication is performed by a stable feature identifier (primary key) that is also exposed as the GeoJSON \texttt{id} when possible; this avoids duplicates when neighbouring cells overlap. After merging, the original attribute and spatial filter are reapplied so the final result matches what PostGIS would have returned for the exact footprint. In practice exact geometry checks (e.g., \texttt{ST\_Intersects}/\texttt{ST\_Within}) are applied on the composed set to trim any over-coverage from boundary cells.

Partial failures are also guarded against. If some cells miss or expire while others hit, the middleware fetches the missing ones from GeoServer and only returns once the full set is assembled. Latencies and correctness counters are tagged by “full-hit/partial/miss” so the impact in P50/P95/P99 can be inspected and post-filtering can be verified to keep answers faithful to the original query.

\subsection{Freshness Mechanisms}
TTL-based freshness is combined with event-driven invalidation. TTL gives a clear freshness lifetime (fresh vs.\ stale) that is easy to reason about and measure. Event-driven invalidation reacts to actual data changes and targets only the overlapping H3 cells, keeping hot regions up to date without broad purges.

\subsubsection{TTL Policy and Configuration}
On write, each key gets a TTL, keys expire automatically once the countdown reaches zero. TTL is set at insert time. Remaining lifetime is tracked with \texttt{TTL}/\texttt{PTTL} during debugging. TTLs are per layer: fast-changing layers get short TTLs, static base layers get longer ones.

Operationally, Redis removes expired keys and can also evict keys under memory pressure according to \texttt{maxmemory-policy}. An \emph{allkeys} policy (LRU/LFU) is used when the dataset is entirely cacheable and memory-bounded, or a \emph{volatile} policy is used when only TTL-tagged entries should be evicted. Metrics for expiry and eviction are also exposed so freshness decisions (TTL) can be separated from memory backpressure effects (evictions) in the results.

\subsubsection{Kafka/CDC Event Processing and Spatial Invalidation}
To cut staleness after updates, change events are consumed from PostgreSQL. The PostgreSQL connector streams inserts/updates/deletes from the WAL as structured messages; the middleware extracts the changed geometry, maps it to H3 cells, and invalidates the matching keys. This targets only the affected spatial scope and avoids global cache clears.

Kafka’s ordering within a partition is relied on: by partitioning events on a stable key (e.g., feature ID), all changes for the same feature arrive in order. Delivery is at-least-once by default \cite{Kafka_4_1_Design}, so the invalidation handler is \emph{idempotent} and \emph{version-aware} (drops events that have already been processed at an equal or newer source LSN). Where a stricter pipeline is needed, Kafka’s idempotent producers and transactions can be used to enable exactly-once processing, with added operational cost. These guarantees keep invalidation consistent even during retries or restarts.

For spatial targeting the H3 cells touched by the changed geometry are computed using the standard polygon-to-cells functions, then keys for those \texttt{<layer,res,cellId,filterHash>} combinations are deleted. This ties freshness directly to spatial overlap while keeping runtime costs low.

\subsection{Experimental Methodology}
The experiments compare an H3-keyed middleware cache against baselines under controlled, repeatable conditions. Each run follows a fixed script: container start, warm-up, timed measurement window, and teardown. Image tags and record digests are pinned to keep versions stable across repetitions. Load is driven by scripted scenarios at a fixed arrival rate so that request mix and intensity are identical between configurations. Runs are repeated multiple times, and then aggregate medians and percentiles across repetitions.

Measurements are taken only during a steady window after warm-up. Latency (P50/P95/P99), hit/miss rates, and backend load (PostGIS CPU, Redis memory/evictions) are collected. External factors are kept stable: hardware, container limits, network, and dataset. The study is comparative rather than absolute, so the study does not claim universal latency numbers, but it does quantify deltas between configurations under the same workload. Threats to validity (synthetic traffic, single-node deployments, and Docker networking) are noted, and they are mitigated by repeating trials with fixed seeds.

\subsubsection{Testbed and Deployment (Containers, Versions, Hardware)}
All components run in Docker on a single host laptop. Images are pinned by tag and the exact image digest is recorded in the repository. The stack includes: GeoServer (WFS/WMS), PostgreSQL, PostGIS, Redis, the middleware proxy, Kafka, Prometheus, Grafana, Loki, and Promtail. Each service is launched via a compose file. The dataset is loaded once at startup, and containers are restarted between configurations to reset caches.

Hardware was a single laptop running Arch Linux (Lenovo Legion 7 15IMH05) with an Intel Core i7-10750H (6 cores / 12 threads) and 16~GiB RAM. The experimental stack ran in Docker containers (PostGIS, GeoServer, Redis, Kafka, and monitoring). Redis was configured with a fixed memory budget using \texttt{maxmemory}=256~MiB and the \texttt{allkeys-lru} eviction policy to bound cache growth. Prometheus scraped service metrics and Grafana dashboards summarized latency distributions, throughput, error rates, and Redis memory/evictions; log-based observability was provided via Loki/Promtail.

\subsubsection{Baselines and Compared Configurations}
Four configurations are compared:
\begin{enumerate}
  \item \textbf{Baseline with low Zipf}: No cache, requests go directly to GeoServer/PostGIS, and the distribution of calls are spread out.
    \item \textbf{Baseline with high Zipf}: No cache, requests go directly to GeoServer/PostGIS, and the distribution of calls are compact.
  \item \textbf{H3 cache with low H3 res and low Zipf}: per-cell keys at low H3 resolution, and the distribution of calls are spread out.
  \item \textbf{H3 cache with high H3 res and low Zipf}: per-cell keys at high H3 resolution, and the distribution of calls are spread out.
  \item \textbf{H3 cache with low H3 res and high Zipf}: per-cell keys at low H3 resolution, and the distribution of calls are compact.
  \item \textbf{H3 cache with high H3 res and high Zipf}: per-cell keys at high H3 resolution, and the distribution of calls are compact.
\end{enumerate}

For H3 resolutions $r\in\{5,6,7,8,9,10\}$ are used to study hit ratio, memory, speed, and correctness. For each configuration the same dataset, arrival pattern, and container limits are kept. End-to-end latency (P50/P95/P99), misses avoided, Redis memory/evictions, and staleness (fraction of responses older than target) are reported.

\subsubsection{Experiment Procedure}
Each trial proceeds as follows:

\begin{enumerate}
  \item Start all containers and wait for health checks to pass.
  \item Warm up the system by running the target request for a fixed duration (e.g., 1 minutes) to stabilise caches and JIT optimisations.
  \item Run the load generator at the target constant arrival rate for a fixed measurement window (e.g., 4 minutes), collecting metrics during this period.
  \item Stop the load generator and export collected metrics from Prometheus and middleware logs for analysis.
  \item Restart containers to reset state before the next configuration.
\end{enumerate}

Repeat this procedure for each configuration and workload, ensuring that each run uses the same request list and arrival pattern. Each configuration is repeated multiple times (e.g., 5 times) to account for variability, and median/percentile metrics are computed across runs.
