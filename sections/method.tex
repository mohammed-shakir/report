%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% -                       Method                           - %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}

\subsection{Research Approach and Study Design}
The goal is to measure how an H3-keyed middleware cache affects latency and load under skewed spatial workloads, compared to no cache. Each configuration is run with the same datasets, the same mix of query footprints (BBOX and polygons), and the same arrival pattern. Experiments are containerised and scripted so runs are repeatable.

For each workload profile we cycle through the configurations, repeat runs, and report median and percentile metrics across repetitions. External factors are fixed as much as possible (hardware, network, versions, etc.), warm up the system, and then collect measurements during a steady window. The SRE “golden signals” are reported and cache-specific indicators: P50/P95/P99 latency (overall, hit-path, miss-path), requests/s, error rate, saturation (CPU/memory), and hit/miss/partial-hit ratios.

Independent variables are H3 resolution (7–9), decay, and freshness policy. Dependent variables are latency percentiles, PostGIS CPU, Redis memory and evictions, and staleness.

Validity measures include repeated trials, fixed seeds for traffic, and cross-checking results. Threats to validity are noted (dataset representativeness, synthetic vs.\ real traffic, and Docker networking differences), and the evaluation is framed as comparative rather than absolute.

\subsection{System Under Study and Assumptions}
The system mirrors a common geospatial setup: GeoServer exposes WFS/WMS endpoints backed by PostGIS, a middleware proxy sits in front and integrates Redis for caching and Kafka for change events. Prometheus scrapes metrics from services. Clients issue WFS \texttt{GetFeature} requests with BBOX or polygon filters and optional attribute filters.

Layers are published in \texttt{EPSG:4326} and responses are encoded as GeoJSON. Queries target urban layers with realistic skew, mixing small viewports (city blocks) with medium windows (districts). The cache key encodes (layer, SRID, resolution, H3 cell ID, filter hash, version). Each value has a TTL and can be invalidated by events. Redis is configured with a bounded memory and an eviction policy \cite{Redis_Eviction}.

Kafka provides in-partition ordering \cite{Kafka_Introduction}; consumers are in a single group per environment so each partition is processed by exactly one consumer. Event handling is repeat-safe so duplicate invalidations are harmless.

We assume stable network when testing, and no background load except the experiment. We do not change GeoServer/PostGIS schemas or indexes; the middleware is non-invasive and standards-compliant on the wire.

\subsection{Middleware Design and Implementation}
The middleware is a Go service that acts as an HTTP reverse proxy with spatial awareness. It parses incoming requests, maps footprints to H3 cells, looks up Redis for per-cell entries, forwards misses to GeoServer, and composes responses. A Kafka consumer listens for change events, maps updated geometries to cells, and deletes affected keys. The service exposes Prometheus metrics and health checks. Configuration is via environment variables and a simple YAML file; all components run in Docker for reproducibility.

\subsubsection{End-to-End Request Flow and Component Responsibilities}
\textbf{1. Router:} Accepts client WFS/WMS requests, validates required parameters (service, request, typeNames, spatial filter), normalises ECQL/CQL filters, and builds an internal QueryRequest. It preserves wire compatibility with GeoServer so clients do not change their integrations.

\textbf{2. H3 mapper:} Converts the footprint (BBOX as polygon, or user polygon) to a set of H3 cell IDs at the chosen resolution. The selection of resolution is static per run.

\textbf{3. Decision and hotness:} Updates per-cell counters (exponential decay). Cells above threshold are eligible for caching; others pass through without admission to reduce turnover. This keeps memory focused on regions with reuse.

\textbf{4. Cache manager:} Builds composite keys (layer/SRID/res/cell/filter-hash/version), performs a single MGET to Redis, and classifies the request as full hit, partial hit, or miss. Values are stored with TTL. Redis is configured with an eviction policy suitable for caches and a fixed maxmemory to force realistic trade-offs.

\textbf{5. Executor:} For cells not found, constructs a WFS \texttt{GetFeature} URL and forwards to GeoServer. The result is parsed, stored per-cell (with TTL), and streamed to the aggregator.

\textbf{6. Aggregator:} Merges per-cell payloads, deduplicates features by stable identifiers, reapplies the original spatial/attribute filter to avoid overfetch, and returns a single GeoJSON response. It records keys-per-request and merge time.

\textbf{7. Invalidation listener:} Subscribes to Kafka topics carrying change events. For each event, maps the changed geometry to H3 cells and issues key deletions. Processing is idempotent, and ordering is ensured within partitions.

\textbf{8. Metrics/health:} Exposes request counters, latency histograms, hit/miss ratios, Redis stats, and invalidation lag. Health endpoints return liveness/readiness for containers.

\subsubsection{H3 Footprint Mapping and Request Bucketing}
We use H3’s polygon-to-cells (“polygonToCells”) functions to map a request footprint to cell IDs \cite{H3_API_Regions}. A BBOX is treated as a polygon with four vertices. For each request, the mapper returns the set of cell IDs at resolution $r$, which become the unit of reuse and invalidation. The default polygonToCells uses a centroid-in-polygon rule \cite{H3_API_Regions}, which is simple and fast; thin shapes near boundaries can be handled by increasing $r$ or by expanding one ring and trimming on compose.

Resolution controls granularity. Higher $r$ gives smaller cells, more precise coverage, and often higher hit probability for small windows, but increases keys per request. Lower $r$ reduces keys and metadata but can over-cover and reduce locality. We therefore sweep $r\in\{7,8,9\}$ in experiments and record hit ratio, keys-per-request, and bytes-per-cell.

The cache key includes the cell ID and a stable hash of the attribute filter (normalised ECQL). This keeps semantically different queries isolated while enabling reuse when both footprint and filter match.

On composed responses we reapply the original spatial filter to remove any features picked up by boundary cells. Multi-polygons are supported by passing outer and inner rings to the polyfill. All cell operations are deterministic so repeated queries map to the same key set.
